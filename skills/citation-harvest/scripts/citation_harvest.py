#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import os
import re
import sys
import urllib.parse
import urllib.request
from pathlib import Path

S2_API = "https://api.semanticscholar.org/graph/v1/paper/search"


def _require_online() -> None:
    if os.getenv("ASV2_ONLINE") != "1":
        raise SystemExit("[ERROR] Offline mode. Re-run with --online to allow network calls.")


def _load_queries(path: Path | None, queries: list[str]) -> list[str]:
    items = list(queries)
    if path:
        text = path.read_text(encoding="utf-8")
        for line in text.splitlines():
            if line.strip():
                items.append(line.strip())
    return items


def _sanitize_key(text: str) -> str:
    text = re.sub(r"[^a-zA-Z0-9]+", "_", text.strip().lower())
    return text.strip("_") or "paper"


def _bibtex_entry(paper: dict) -> str:
    title = paper.get("title", "Untitled")
    year = paper.get("year", "")
    authors = paper.get("authors", [])
    author_str = " and ".join([a.get("name", "") for a in authors if a.get("name")])
    venue = paper.get("venue", "")
    key = _sanitize_key(f"{title}_{year}")
    fields = {
        "title": title,
        "author": author_str,
        "year": str(year),
        "journal": venue,
    }
    body = ",\n".join([f"  {k}={{" + v.replace("{", "").replace("}", "") + "}}" for k, v in fields.items() if v])
    return f"@article{{{key},\n{body}\n}}"


def _fetch(query: str, limit: int, fields: str) -> list[dict]:
    params = {
        "query": query,
        "limit": str(limit),
        "fields": fields,
    }
    url = f"{S2_API}?{urllib.parse.urlencode(params)}"
    req = urllib.request.Request(url)
    api_key = os.getenv("S2_API_KEY")
    if api_key:
        req.add_header("x-api-key", api_key)
    with urllib.request.urlopen(req, timeout=60) as resp:
        data = json.loads(resp.read().decode("utf-8"))
    return data.get("data", [])


def main() -> int:
    ap = argparse.ArgumentParser(description="Harvest citations from Semantic Scholar.")
    ap.add_argument("--in", dest="in_path", help="Path to text file with one query per line.")
    ap.add_argument("--query", action="append", default=[], help="Query string (repeatable).")
    ap.add_argument("--limit", type=int, default=5, help="Results per query.")
    ap.add_argument("--out-json", required=True, help="Output JSON file.")
    ap.add_argument("--out-bib", required=True, help="Output BibTeX file.")
    ap.add_argument("--online", action="store_true", help="Enable network calls.")
    args = ap.parse_args()

    if args.online:
        os.environ["ASV2_ONLINE"] = "1"
    _require_online()

    in_path = Path(args.in_path) if args.in_path else None
    queries = _load_queries(in_path, args.query)
    if not queries:
        print("[ERROR] No queries provided.", file=sys.stderr)
        return 2

    all_results: list[dict] = []
    seen = set()
    fields = "title,authors,venue,year,externalIds,citationCount,url"
    for q in queries:
        for paper in _fetch(q, args.limit, fields):
            ext = paper.get("externalIds") or {}
            key = ext.get("DOI") or (paper.get("title", "").lower(), paper.get("year"))
            if key in seen:
                continue
            seen.add(key)
            paper["query"] = q
            all_results.append(paper)

    out_json = Path(args.out_json)
    out_json.write_text(json.dumps({"queries": queries, "results": all_results}, indent=2), encoding="utf-8")

    bib_entries = ["% Generated by citation_harvest.py"]
    for paper in all_results:
        bib_entries.append(_bibtex_entry(paper))
    Path(args.out_bib).write_text("\n\n".join(bib_entries), encoding="utf-8")

    print(f"[OK] Wrote {out_json} and {args.out_bib}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
